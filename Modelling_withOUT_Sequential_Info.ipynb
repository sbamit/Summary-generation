{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JMcPW9HUgOz"
      },
      "source": [
        "# Machine Learning Implementation (Without Sequential Information)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to GoogleDrive"
      ],
      "metadata": {
        "id": "8P1mcULGME1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU76oJu0Ls8w",
        "outputId": "22e6b025-bf7f-4ca8-c6a9-71eeee811579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/CIS5930_Project/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpZMWiBeL4Yo",
        "outputId": "650580ef-6936-495b-c6e9-5dbe32ce8343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1K9uiTsAwFaNNqySxib53SuBRuxArSWES/CIS5930_Project\n",
            "'Arunima-CIS 5930 Project: Data Wrangling.ipynb'\n",
            "'CIS 5930 Project: Data Wrangling.ipynb'\n",
            "'CIS 5930 Project: ML Model.ipynb'\n",
            "'CIS 5930 Project: ML Results.ipynb'\n",
            " cv_results_LEDE3.pickle\n",
            " cv_results_lstm_bi25_embeddings_only_epochs1_top3.pickle\n",
            " cv_results_lstm_bi50_embeddings_only_epochs1_top3.pickle\n",
            " cv_results_lstm_bi75_embeddings_only_epochs1_top3.pickle\n",
            " cv_results_lstm_uni25_embeddings_only_epochs1_top3.pickle\n",
            " cv_results_lstm_uni50_embeddings_only_epochs1_top3.pickle\n",
            " cv_results_nn2525_embeddings_only_cw_top3_epochs50.pickle\n",
            " cv_results_nn2550_embeddings_only_cw_top3_epochs50.pickle\n",
            " cv_results_nn5050_embeddings_only_cw_top3_epochs50.pickle\n",
            " cv_results_textrank.pickle\n",
            " cv_results_top_3_logreg_cw_balanced.pickle\n",
            " cv_results_top_3_logreg_default.pickle\n",
            " cv_results_top_3_logreg_elasticnet_gridsearch.pickle\n",
            " cv_results_top_3_logreg_elasticnet.pickle\n",
            " cv_results_top_3_logreg_sent_num_bal.pickle\n",
            " cv_results_top_3_logreg_sent_num_no_bal.pickle\n",
            " dev-stats.jsonl\n",
            " extractive_all_domain_labels.pickle\n",
            " functions.py\n",
            " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            " test-stats.jsonl\n",
            " train_stats_df_extractive_no_spacy.pickle\n",
            " train_stats_df_no_spacy.pickle\n",
            " train_stats_df_processed_extr_5000.pickle\n",
            " train_stats_df_processed_extr_label_5000.pickle\n",
            " train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle\n",
            " train_stats_dict_processed_extr_final_5000_.pickle\n",
            " train-stats.jsonl\n",
            " train_test_set20_embeddings_only.pickle\n",
            " train_test_set20_embeddings_sent_num.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the dependencies "
      ],
      "metadata": {
        "id": "zXTIJvCDEFCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKVYgJnyLRxE",
        "outputId": "3672c9de-d1f6-40b4-8975-ad2c4114092f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQezD2lUgO2"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyR7TVlUgO2"
      },
      "source": [
        "return_df_pred_summaries_: returns the predicted summaries given the fixed number of sentences required "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUa4omRyUgO3"
      },
      "outputs": [],
      "source": [
        "###Sub-function used in return_pred_summaries\n",
        "\n",
        "def return_greater_than_min_num(arr, thresh=0.5, min_num=1, fix_num_flag=False, fix_num=3):\n",
        "    \n",
        "    '''returns top sentences by index numbers in ascending format and according to input\n",
        "    specifications\n",
        "    '''\n",
        "    #want fixed number sentences?\n",
        "    if fix_num_flag == True:\n",
        "        idx = np.argsort(arr)[-fix_num:]\n",
        "        \n",
        "    #return above model threshold only    \n",
        "    else:\n",
        "        idx_prelim = np.where(arr>= thresh)\n",
        "        \n",
        "        #filter for minimum number required\n",
        "        if idx_prelim[0].shape[0] <= min_num:\n",
        "            idx = np.argsort(arr)[-min_num:]\n",
        "        else:\n",
        "            idx = idx_prelim\n",
        "            \n",
        "    #return in ascending order\n",
        "    return sorted(idx)\n",
        "\n",
        "\n",
        "###Main helper function    \n",
        "def return_df_pred_summaries( Xy_doc_label, y_pred, df_text, thresh, min_num,\n",
        "                             return_all=False, fix_num_flag=False, fix_num=3):\n",
        "    \n",
        "    '''return list of predicted summaries and additional information if required\n",
        "    and according to inout specifications'''\n",
        "    \n",
        "    #Wrangle to doc label and flattened array of predictions for each article\n",
        "    df_label_pred = pd.DataFrame({'doc_label': Xy_doc_label.flatten(),\n",
        "                                                 'y_pred': y_pred.flatten()}) \n",
        "    df_label_pred = df_label_pred.groupby('doc_label').agg(list) \n",
        "\n",
        "    df_label_pred = df_label_pred.applymap(lambda x: np.array(x))\n",
        "\n",
        "    #subfunction to lambda\n",
        "    f = lambda arr: return_greater_than_min_num(arr, thresh=thresh, \n",
        "                                    min_num=min_num,fix_num_flag = fix_num_flag, \n",
        "                                                            fix_num=fix_num)\n",
        "    #get sorted index sentence numbers to include in article\n",
        "    df_label_pred = df_label_pred.applymap(f) \n",
        "\n",
        "    #Return predicted summary\n",
        "          #index is doc label\n",
        "    df_doc = df_text[df_label_pred.index]     \n",
        "    \n",
        "          # return article sentences as list\n",
        "    pred_summaries = [np.array(df_doc.iloc[j])       \n",
        "                               [df_label_pred.iloc[j][0]].tolist()                      \n",
        "                                          for j in range(len(df_label_pred))]\n",
        "          #join into summary as single string\n",
        "    pred_summaries = [summ_list if type(summ_list) == str else   \n",
        "                      ' '.join(summ_list) for summ_list in pred_summaries]  \n",
        "    \n",
        "    if return_all == True:\n",
        "        answer = df_label_pred.values, df_label_pred.index, pred_summaries\n",
        "    else:\n",
        "        answer = pred_summaries\n",
        "    \n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tY2WwggUgO5"
      },
      "source": [
        "calc_rouge_scores: calculates average Rouge scores across multiple predicted and gold summary pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fzAOI87UgO6"
      },
      "source": [
        "## Supervised Learning Using Only Embedding Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJwUvKRGUgO6"
      },
      "source": [
        " ### 1 Logistic Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8YnU8YmUgO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56b0410-7811-4bd9-b2a0-9a1999aa2a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-19 14:37:56.739148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': {'recall': 0.3959067671907968, 'precision': 0.5779447801266966, 'f1': 0.41302032283664225}, 'rougeL': {'recall': 0.35069379180120924, 'precision': 0.5178247108721542, 'f1': 0.36775040341776893}}\n",
            "2022-04-19 14:43:19.155329\n",
            "0:05:22.416181\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from functions import return_df_pred_summaries\n",
        "from functions import calc_rouge_scores\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "input_filename = 'train_test_set20_embeddings_only.pickle'\n",
        "\n",
        "#output_file =  'cv_results_top_3_logreg_default.pickle'\n",
        "#output_file = 'cv_results_top_3_logreg_cw_balanced.pickle'\n",
        "output_file = 'cv_results_top_3_logreg_elasticnet.pickle'\n",
        "\n",
        "\n",
        "t1 = dt.now()\n",
        "print(t1)\n",
        "\n",
        "data_dict = pd.read_pickle(input_filename)\n",
        "\n",
        "#Specify model inputs: df, X, y, doc_labels\n",
        "df = data_dict['df_original']\n",
        "train_test_set = data_dict['train_test_sets']\n",
        "#Specify train-test_data for validation        \n",
        "Xy_doc_label_train = train_test_set[0][0]\n",
        "Xy_doc_label_test = train_test_set[0][1]\n",
        "X_train = train_test_set[0][2]\n",
        "X_test = train_test_set[0][3]\n",
        "y_train = train_test_set[0][4]\n",
        "y_test = train_test_set[0][5]\n",
        "\n",
        "#Define Model\n",
        "#LogisticRegression(random_state=42)\n",
        "#LogisticRegression(class_weight='balanced', random_state=42)\n",
        "model = LogisticRegression(solver='saga', penalty='elasticnet',\n",
        "                           l1_ratio=0.25, C=0.5, random_state=42)\n",
        "#Fit model\n",
        "model.fit(X_train,y_train)\n",
        "#Predict Model\n",
        "y_pred = model.predict_proba(X_test)\n",
        "    \n",
        "#Convert to binary predictions\n",
        "y_pred_bin = (y_pred >=0.5)*1\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_bin[:,1], labels=[0,1])\n",
        "\n",
        "#Return predicted summaries\n",
        "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
        "                                y_pred[:,1], df.text_clean, thresh=0.5, min_num=1, \n",
        "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
        "\n",
        "#Match with gold summaries\n",
        "df_gold = df.summary_clean[doc_index]\n",
        "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
        "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
        "\n",
        "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
        "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
        "               'sent_index_number': idx, 'Rouge': scores}\n",
        "\n",
        "with open(output_file, 'wb') as handle:                                     \n",
        "    pickle.dump(results_dict, handle)\n",
        "\n",
        "print(scores)\n",
        "\n",
        "t2 = dt.now()\n",
        "print(t2)\n",
        "print(t2-t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHokllFUgO9"
      },
      "source": [
        " ### 2 Neural Net Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ9eHwcdUgO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dfe454-dce3-4568-b717-ccd693b5d1e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-19 15:57:01.256561\n",
            "Epoch 1/50\n",
            "3807/3807 [==============================] - 12s 3ms/step - loss: 0.8441 - sensitivity_at_specificity: 0.2817 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 2/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.8110 - sensitivity_at_specificity: 0.3587 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 3/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7986 - sensitivity_at_specificity: 0.3801 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 4/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7908 - sensitivity_at_specificity: 0.3866 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 5/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7819 - sensitivity_at_specificity: 0.4062 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 6/50\n",
            "3807/3807 [==============================] - 12s 3ms/step - loss: 0.7770 - sensitivity_at_specificity: 0.4176 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 7/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7703 - sensitivity_at_specificity: 0.4256 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 8/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7659 - sensitivity_at_specificity: 0.4382 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 9/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7613 - sensitivity_at_specificity: 0.4490 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 10/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7576 - sensitivity_at_specificity: 0.4527 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 11/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7546 - sensitivity_at_specificity: 0.4654 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 12/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7499 - sensitivity_at_specificity: 0.4727 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 13/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7459 - sensitivity_at_specificity: 0.4765 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 14/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7432 - sensitivity_at_specificity: 0.4811 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 15/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7415 - sensitivity_at_specificity: 0.4807 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 16/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7370 - sensitivity_at_specificity: 0.4903 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 17/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7343 - sensitivity_at_specificity: 0.4971 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 18/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7315 - sensitivity_at_specificity: 0.5009 - specificity_at_sensitivity: 0.8624\n",
            "Epoch 19/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7292 - sensitivity_at_specificity: 0.5000 - specificity_at_sensitivity: 0.0000e+00\n",
            "Epoch 20/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7281 - sensitivity_at_specificity: 0.5122 - specificity_at_sensitivity: 0.8565\n",
            "Epoch 21/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7255 - sensitivity_at_specificity: 0.5101 - specificity_at_sensitivity: 0.8625\n",
            "Epoch 22/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7240 - sensitivity_at_specificity: 0.5161 - specificity_at_sensitivity: 0.8604\n",
            "Epoch 23/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7220 - sensitivity_at_specificity: 0.5202 - specificity_at_sensitivity: 0.8578\n",
            "Epoch 24/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7197 - sensitivity_at_specificity: 0.5267 - specificity_at_sensitivity: 0.8566\n",
            "Epoch 25/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7173 - sensitivity_at_specificity: 0.5251 - specificity_at_sensitivity: 0.8572\n",
            "Epoch 26/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7150 - sensitivity_at_specificity: 0.5304 - specificity_at_sensitivity: 0.8577\n",
            "Epoch 27/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7152 - sensitivity_at_specificity: 0.5300 - specificity_at_sensitivity: 0.8570\n",
            "Epoch 28/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7115 - sensitivity_at_specificity: 0.5357 - specificity_at_sensitivity: 0.8565\n",
            "Epoch 29/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7100 - sensitivity_at_specificity: 0.5360 - specificity_at_sensitivity: 0.8563\n",
            "Epoch 30/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7069 - sensitivity_at_specificity: 0.5409 - specificity_at_sensitivity: 0.8548\n",
            "Epoch 31/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7075 - sensitivity_at_specificity: 0.5372 - specificity_at_sensitivity: 0.8568\n",
            "Epoch 32/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.7062 - sensitivity_at_specificity: 0.5439 - specificity_at_sensitivity: 0.8550\n",
            "Epoch 33/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7048 - sensitivity_at_specificity: 0.5456 - specificity_at_sensitivity: 0.8539\n",
            "Epoch 34/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7008 - sensitivity_at_specificity: 0.5563 - specificity_at_sensitivity: 0.8527\n",
            "Epoch 35/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.7001 - sensitivity_at_specificity: 0.5526 - specificity_at_sensitivity: 0.8524\n",
            "Epoch 36/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6998 - sensitivity_at_specificity: 0.5579 - specificity_at_sensitivity: 0.8507\n",
            "Epoch 37/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6966 - sensitivity_at_specificity: 0.5575 - specificity_at_sensitivity: 0.8527\n",
            "Epoch 38/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6955 - sensitivity_at_specificity: 0.5557 - specificity_at_sensitivity: 0.8523\n",
            "Epoch 39/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.6941 - sensitivity_at_specificity: 0.5657 - specificity_at_sensitivity: 0.8518\n",
            "Epoch 40/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6947 - sensitivity_at_specificity: 0.5640 - specificity_at_sensitivity: 0.8507\n",
            "Epoch 41/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6920 - sensitivity_at_specificity: 0.5658 - specificity_at_sensitivity: 0.8512\n",
            "Epoch 42/50\n",
            "3807/3807 [==============================] - 10s 3ms/step - loss: 0.6909 - sensitivity_at_specificity: 0.5701 - specificity_at_sensitivity: 0.8471\n",
            "Epoch 43/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6896 - sensitivity_at_specificity: 0.5700 - specificity_at_sensitivity: 0.8496\n",
            "Epoch 44/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6883 - sensitivity_at_specificity: 0.5748 - specificity_at_sensitivity: 0.8489\n",
            "Epoch 45/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6869 - sensitivity_at_specificity: 0.5798 - specificity_at_sensitivity: 0.8468\n",
            "Epoch 46/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6864 - sensitivity_at_specificity: 0.5729 - specificity_at_sensitivity: 0.8491\n",
            "Epoch 47/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6846 - sensitivity_at_specificity: 0.5847 - specificity_at_sensitivity: 0.8435\n",
            "Epoch 48/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6833 - sensitivity_at_specificity: 0.5821 - specificity_at_sensitivity: 0.8472\n",
            "Epoch 49/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6816 - sensitivity_at_specificity: 0.5858 - specificity_at_sensitivity: 0.8458\n",
            "Epoch 50/50\n",
            "3807/3807 [==============================] - 11s 3ms/step - loss: 0.6810 - sensitivity_at_specificity: 0.5849 - specificity_at_sensitivity: 0.8472\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 50)                76850     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,451\n",
            "Trainable params: 79,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "{'rouge1': {'recall': 0.39330563972855165, 'precision': 0.5493390913659438, 'f1': 0.39923807391343613}, 'rougeL': {'recall': 0.34714481002026254, 'precision': 0.49018148139276324, 'f1': 0.3544682100255658}}\n",
            "2022-04-19 16:06:33.712623\n",
            "0:09:32.456062\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from functions import return_df_pred_summaries\n",
        "from functions import calc_rouge_scores\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from datetime import datetime as dt\n",
        "\n",
        "input_filename = 'train_test_set20_embeddings_only.pickle'\n",
        "\n",
        "#output_file = 'cv_results_nn2525_embeddings_only_cw_top3_epochs50.pickle'\n",
        "#output_file = 'cv_results_nn2550_embeddings_only_cw_top3_epochs50.pickle'\n",
        "output_file = 'cv_results_nn5050_embeddings_only_cw_top3_epochs50.pickle'\n",
        "\n",
        "\n",
        "t1 = dt.now()\n",
        "print(t1)\n",
        "\n",
        "data_dict = pd.read_pickle(input_filename)\n",
        "\n",
        "#Specify model inputs: df, X, y, doc_labels\n",
        "df = data_dict['df_original']\n",
        "train_test_set = data_dict['train_test_sets']\n",
        "#Specify train-test_data for validation        \n",
        "Xy_doc_label_train = train_test_set[0][0]\n",
        "Xy_doc_label_test = train_test_set[0][1]\n",
        "X_train = train_test_set[0][2]\n",
        "X_test = train_test_set[0][3]\n",
        "y_train = train_test_set[0][4]\n",
        "y_test = train_test_set[0][5]\n",
        "\n",
        "#class_weights for imbalanced data\n",
        "pos_w = int(y_train.shape[0] / sum(y_train==1)[0])\n",
        "weight_dict = {0:1, 1: pos_w/2}\n",
        "   \n",
        "#Define Model\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=1536, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "#Compile Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
        "metrics=[tf.keras.metrics.SensitivityAtSpecificity(0.5, num_thresholds=1),\n",
        "             tf.keras.metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)])\n",
        "#Fit Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
        "                          callbacks=[callback], class_weight=weight_dict) #class_weight=weight_dict\n",
        "#Predict Model\n",
        "y_pred = model.predict(X_test)\n",
        "    \n",
        "#Convert to binary predictions\n",
        "y_pred_bin = (y_pred >=0.5)*1\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_bin, labels=[0,1])\n",
        "\n",
        "\n",
        "#Return predicted summaries\n",
        "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
        "                                y_pred, df.text_clean, thresh=0.5, min_num=1, \n",
        "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
        "\n",
        "#pred_summaries = [' '.join(df.text[doc_index].iloc[j][:3]) for j in range(len(idx))]\n",
        "\n",
        "#Match with gold summaries\n",
        "df_gold = df.summary_clean[doc_index]\n",
        "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
        "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
        "\n",
        "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
        "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "\n",
        "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
        "               'sent_index_number': idx, 'Rouge': scores, 'mod_summary': model.summary()}\n",
        "\n",
        "with open(output_file, 'wb') as handle:                                     \n",
        "    pickle.dump(results_dict, handle)\n",
        "\n",
        "print(scores)\n",
        "\n",
        "t2 = dt.now()\n",
        "print(t2)\n",
        "print(t2-t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1BRHREUgO9"
      },
      "source": [
        " ### 3 TextRank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvNdMZuUgO-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from datetime import datetime as dt\n",
        "\n",
        "t1 = dt.now()\n",
        "print(t1)\n",
        "\n",
        "output_file = 'train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle' \n",
        "input_file = 'train_stats_dict_processed_extr_final_5000_.pickle' \n",
        "data = pd.read_pickle(input_file )\n",
        "\n",
        "#Select sentence embeddings only and match to doc label\n",
        "df_embed = data['df_X'].loc[:,'Sent_BERT_D_0': 'Sent_BERT_D_767']\n",
        "df_doc_label = pd.DataFrame(data['Xy_doc_label_array'],columns=['doc_label'])\n",
        "df = pd.concat([df_doc_label, df_embed], axis=1)\n",
        "\n",
        "#loop through articles (docs)\n",
        "pagerank_scores_list=[]\n",
        "error_list = []\n",
        "doc_num = np.max(data['Xy_doc_label_array']) \n",
        "for j in range(doc_num+1):\n",
        "    \n",
        "    #calculate cosine similiarity matrix \n",
        "    df_doc = df [df.doc_label == j].iloc[:,2:]\n",
        "    n = df_doc.shape[0]\n",
        "    cos_matrix = cosine_similarity(df_doc, df_doc)\n",
        "    f = np.vectorize(lambda x: 0 if x == 1 else 1)\n",
        "    not_eye = f(np.eye(n,n))\n",
        "    cos_matrix = cos_matrix * not_eye\n",
        "    \n",
        "    #Convert to nx graph\n",
        "    graph = nx.from_numpy_array(cos_matrix)\n",
        "    \n",
        "    #Calculate sentence scores and record error docs\n",
        "    try:\n",
        "        scores_arr = np.array(list(nx.pagerank(graph, max_iter=500).values()))\n",
        "    except:\n",
        "        scores_arr = np.nan\n",
        "        error_list.append(j)\n",
        "   \n",
        "    pagerank_scores_list.append(scores_arr)\n",
        "    \n",
        "pagerank_scores_arr = np.array(pagerank_scores_list)\n",
        "\n",
        "#store in primary dictionary\n",
        "data.update({'textrank_scores_arr_per_doc':pagerank_scores_arr })\n",
        "\n",
        "#save to pickle\n",
        "with open(output_file, 'wb') as handle:                                     \n",
        "    pickle.dump(data, handle)\n",
        "\n",
        "t2=dt.now()\n",
        "print(t2)\n",
        "print(t2-t1)\n",
        "\n",
        "#runtime 4mins50sec for 5000 docs / 29 errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d24XUXkQUgO-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from functions import calc_rouge_scores\n",
        "\n",
        "\n",
        "input_textrank = 'train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle'\n",
        "input_test_labels = 'train_test_set20_embeddings_only.pickle'\n",
        "output_file = 'cv_results_textrank.pickle'\n",
        "\n",
        "data = pd.read_pickle(input_textrank )\n",
        "\n",
        "test_labels = pd.read_pickle(input_test_labels)\n",
        "test_labels = set(test_labels['train_test_sets'][0][1].flatten())\n",
        "\n",
        "#original df with columns including article / summary text\n",
        "df = data['df_original']\n",
        "#add pagerank scores to df\n",
        "df['textrank_scores'] = data['textrank_scores_arr_per_doc']\n",
        "#filter for test set\n",
        "df = df[df.index.isin(test_labels)]\n",
        "#drop where textrank had errors\n",
        "df = df.dropna()\n",
        "\n",
        "#pick top3 sentence by textrank score\n",
        "df['idx'] = df['textrank_scores'].apply(lambda x: sorted(np.argsort(x)[-3:])).values\n",
        "idx_arr = df['idx'].values\n",
        "\n",
        "#convert list of sentences to string for each predicted summary\n",
        "pred_summaries = [' '.join(np.array(df.text_clean.iloc[j])[idx_arr[j]].tolist())\n",
        "                  for j in range(len(idx_arr))]\n",
        "\n",
        "#convert cleaned gold summarysentence lists to string for each summary\n",
        "df_gold = df.summary_clean\n",
        "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
        "\n",
        "#zip each predicted / gold summary pair together and store in another tuple\n",
        "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
        "\n",
        "#calculate rouge scores\n",
        "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
        "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
        "#store results in dict\n",
        "results_dict = {'Rouge': scores, 'doc_labels': df.index.tolist(),\n",
        "                'summaries_comp': summaries_comp}\n",
        "#add to primary dict\n",
        "data.update(results_dict)\n",
        "\n",
        "#save to pickle\n",
        "with open(output_file, 'wb') as handle:                                     \n",
        "    pickle.dump(data, handle)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Submit_CIS5930 Project: Modelling without Sequential Info.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8P1mcULGME1a",
        "zXTIJvCDEFCo",
        "bJwUvKRGUgO6",
        "lmHokllFUgO9",
        "8Y1BRHREUgO9"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}